---
layout: post
title:  "Machine Learning: From the Ground Up"
date:   2015-10-21 11:00:00
categories: science
---

Computers are pretty incredible. They do so much for us and are so intertwined with our day to day activities, that in many ways they seem like magic. Even for scientists, it's a common conceit that a modern computer is so complicated, we don't _really_ know how they work. There are so many layers of abstraction, with whole fields of study dedicated to each layer, that it seems impossible that any one person could understand it all, which is a pretty incredible thing to say about something that a good percentage of the world carries around in their pocket. 

I often feel this way myself, but have been fortunate to have taken a rather circuitous route through academia, getting a taste of many of the levels. Just to keep it all straight in my own head, and maybe to be of use to others, I've decided to try and do my best and write down how I understand a computer actually works, from atoms to a high-level programming language. By it's nature, this is likely doomed to oversimplification, but what the heck, we might discover some neat things along the way. Forgive me ahead of time if I gloss over things that are very important, that people have spent their whole lives studying, or let me know what I've missed / how I can improve it :).

Given that computers can do almost anything by design, I've decided to narrow it down to just looking at my current use for them, implementing *machine learning* algorithms. There's a neat meta aspect to this, which is that the goal of these algorithms is to mimic and analyze the dynamics of natural events, physical phenomena. We approximate these phenomena with mathematical functions, which we then approximate with the physical phenomena available to us inside the computer. In this way, it's kind of all just physics, with the goal being to find a good approximation between the transitions of state that we can control and measure in a computer, and the transitions of state that we observe in the outside world.

From that perspective, I'm going to focus on each level of computation in terms of what "_states_" does it have available to it, what transitions does it allow between those states, and mathematical approximation corresponds to computation done by transitioning between those states. When you look at it like this, everything in a computer is physical, and everything physical is computing. 

Levels of abstraction:
+ Atoms (electrons and nuclei)
+ Solids (bonding and conduction)
+ Devices (wires, diodes, and transistors)
+ State Circuits (latches, flip-flops, registers, memory)
+ Functional Circuits (fused-multiply-add)
+ Assembly Language
+ Imperative Languages (C)
+ Object Oriented Languages (python)